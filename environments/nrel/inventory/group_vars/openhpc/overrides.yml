openhpc_install_type: generic # i.e. don't want OpenHPC
openhpc_packages_extra_nrel: # extra indirection to allow lab environment to filter this
  - bzip2
  - curl
  - fio
  - libaio
  - libaio-devel
  - git
  - gzip
  - make
  - patch
  - tar
  - unzip
  - wget
  - xz
  - zstd
  - tmux
  - vim
  - nano
  - python39-devel

  - slurm-libpmi-ohpc
  - slurm-sview-ohpc
  - slurm-libpmi-ohpc
  - docs-ohpc
  - EasyBuild-ohpc
  - ohpc-autotools
  - cmake-ohpc
  - hwloc-ohpc
  - singularity-ohpc
  - slurm-libpmi-ohpc
  - slurm-sview-ohpc
  - slurm-libpmi-ohpc
  - ohpc-base
  - ohpc-base-compute
  - ohpc-slurm-client
  - lmod-ohpc
  - nhc-ohpc
  - valgrind-ohpc
  - clustershell
  - conman-ohpc
  - magpie-ohpc
  - charliecloud-ohpc



  # - gnu9-compilers-ohpc
  # - openmpi4-gnu9-ohpc
  # - intel-compilers-devel-ohpc
  # - intel-oneapi-toolkit-release-ohpc

  # impi-devel
  # - intel-mpi-devel-ohpc
  # - mpich-ofi-intel-ohpc
  # - mpich-ucx-intel-ohpc
  # - mvapich2-intel-ohpc
  # - openmpi4-intel-ohpc
  # - openmpi4-gnu9-ohpc

  # #openmpi-*-ohpc mvapich2-*-ohpc intel-mpi-ohpc



  #- mpich-ofi-gnu9-ohpc
  #- mpich-ucx-gnu9-ohpc

  # - mvapich2-gnu9-ohpc
  # - hpc-gnu9-openmpi4-io-libs
  # - ohpc-gnu9-openmpi4-perf-tools
  # - ohpc-gnu9-openmpi4-parallel-libs

  # - ucx-ohpc
  # - ohpc-gnu9-mpich-perf-tools
  # - ohpc-gnu9-mpich-io-libs
  # - ohpc-gnu9-mpich-parallel-libs
  # - ohpc-gnu9-mpich-perf-tools


  # - hdf5-gnu9-ohpc
  # - netcdf-gnu9-impi-ohpc
  # - netcdf-gnu9-mpich-ohpc
  # - netcdf-gnu9-mvapich2-ohpc
  # - netcdf-gnu9-openmpi4-ohpc

  #- phdf5-gnu9-impi-ohpc
  #- phdf5-gnu9-mpich-ohpc
  #- phdf5-gnu9-mvapich2-ohpc
  #- phdf5-gnu9-openmpi4-ohpc

  #- pnetcdf-gnu9-impi-ohpc
  #- pnetcdf-gnu9-mpich-ohpc
  #- pnetcdf-gnu9-mvapich2-ohpc
  #- pnetcdf-gnu9-openmpi4-ohpc
  #- openblas-gnu9-ohpc
  #- boost-gnu9-impi-ohpc
  #- boost-gnu9-mpich-ohpc
  #- boost-gnu9-mvapich2-ohpc
  #- boost-gnu9-openmpi4-ohpc

# - python3
# - make

# system package installs - generic slurm
openhpc_generic_packages:
  # below are default in the role, required to get slurm to work:
  - munge
  - mariadb-connector-c # only actually needed on slurmdbd
  - hwloc-libs          # only actuall needed on slurmd
  # below added here to get pingpong and pingmatrix hpctests working:
  - mpitests-openmpi

# Additional parameters to set in slurm.conf - use yaml format
openhpc_slurmd_spool_dir: /var/spool/slurm/slurmd
openhpc_config_extra:
  LaunchParameters: use_interactive_step
  FirstJobId: '50000000'
  PropagateResourceLimits: 'NONE'
  # vs cgid
  # ProctrackType: proctrack/cgroup
  ReturnToService: '1'
  SlurmctldPidFile: /var/run/slurmctld.pid
  SlurmdPidFile: /var/run/slurmd.pid
  SwitchType: 'switch/none'
  TaskPlugin: 'task/affinity'
  # TaskPlugin: 'task/affinity,task/cgroup'

  # Use sbatch or srun, however...
  # Don't recomend salloc, but this makes it less troublesome
  #### This fails kbendl - 2021-07-05
  # SallocDefaultCommand: 'srun  --pty --gres=NONE --preserve-env $SHELL'

  # PROLOGUE & EPILOGUE
  # JobSubmitPlugins: 'require_timelimit,lua'
  # PrologSlurmctld: '/nopt/slurm/etc/prologslurmctld.sh'
  # Prolog: '/nopt/slurm/etc/prolog.d/*'
  # PrologFlags: 'X11'
  # X11Parameters: 'local_xauthority'
  # Epilog: '/nopt/slurm/etc/epilog.d/*'
  # PrologEpilogTimeout: 180
  # UnkillableStepTimeout: 180

  # FairShare/FairTree Halflife decay, starting at 14 days on Oct 5th 2020:
  PriorityDecayHalfLife: '14-0'

  # PRIORITY
  # WHB on 2021-05-03 ref: HPCOPS-1447
  # to rebalance things to give a small (4%) weight to job age:
  #   fairshare 52%
  #   QoS 27%
  #   jobsize 12%
  #   partition = 5%
  #   Age 4%

  PriorityType: 'priority/multifactor'
  PriorityMaxAge: '14-0'
  PriorityWeightFairshare: '100000'
  PriorityWeightAge: '1000'
  PriorityWeightJobSize: '1000'
  #PriorityWeightQOS: '206477100'
  PriorityWeightPartition: '38236500'

  # #FastSchedule=0
  # #PriorityUsageResetPeriod=14-0
  # PriorityWeightFairshare=100000
  # PriorityWeightAge=1000
  # # PriorityWeightPartition=  #
  # # larger the job, priority relative to fairshare per cpu/cores
  # PriorityWeightJobSize=1000
  # PriorityMaxAge=4-0
  # PreemptType=preempt/partition_prio
  # PreemptMode=SUSPEND,GANG


  # TIMERS  size because db on spinning disk so had to increase
  BatchStartTimeout: '30'
  MessageTimeout: '100'
  TCPTimeout: '10'

  # SCHEDULING
  SchedulerType: 'sched/backfill'
  SelectType: 'select/cons_tres'
  SelectTypeParameters: 'CR_Core'
  EnforcePartLimits: 'ALL'
  SchedulerParameters:
    - defer
    - default_queue_depth=10750
    - max_rpc_cnt=125
    - max_sched_time=6
    - partition_job_depth=10500
    - sched_max_job_start=200
    - sched_min_interval=2000000
    - batch_sched_delay=20
    - bf_max_job_test=13000
    - bf_interval=30
    - bf_continue
    - bf_window=15840
    - bf_resolution=250
    - max_switch_wait=172800
  #FastSchedule=1

# LOGGING
  SlurmctldDebug: 'info'
  SlurmctldLogFile: '/var/log/slurmctld.log'
  SlurmdDebug: 'info'

# ACCOUNTING
### start with a qos
  AccountingStorageEnforce:
    - safe
    - qos
    - limits
  AcctGatherNodeFreq: '30'
  AccountingStorageTRES: 'gres/gpu'

## added kmb
  # AcctGatherFilesystemType: 'acct_gather_filesystem/lustre'
  # JobAcctGatherType: 'jobacct_gather/linux'
  # JobAcctGatherParams: 'UsePss'
  # JobAcctGatherFrequency: 'task=30,energy=30'

  ### NREL:
  # Kurt... Need to build this - may need the mellanox info here.
  #NHC Node Health Check
  HealthCheckProgram: '/usr/sbin/nhc'
  HealthCheckInterval: 300
  # - HealthCheckNodeState: 'CYCLE,ANY'

# kbendl - the ephemeral mount for SSD?
  TmpFS: '/var/scratch'
  GresTypes: 'gpu'
  ### - MpiDefault: "pmi2"    ### get the launcher to work! pmi1, pmi2, or pmix
  # # spack mpi drivers were a mis-match... is this working now????

  # # NREL: see topology.conf
  # - 'TopologyPlugin: 'topology/tree'
  # - 'JobRequeue: 0
  # - 'MaxJobCount: 40000

